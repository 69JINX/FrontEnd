üëâüèªNodejs
Nodejs is the code written in c++ and javascript. Nodejs runtime unlike the browser runtime does not have access to the web APIs, there is no window or document when working with Nodejs
Node.js is an open-source, cross-platform Javascript runtime environment.
It is not a language, it is not a framework. It is a runtime environment.
Capable of executing JS code outside a browser. 
It can execute not only the standard ECMAScript language but also new features that are made available through C++ bindings using the V8 engine
It consists of C++ files which form the core features and javascript files which expose common utilities and some of the C++ features for easier consumption

With Nodejs, we can create :
1. Traditional websites
2. Backend services like APIs
3. Real-time applications
4. Streaming services
5. CLI tools
6. Multiplayer games

lets take file system as an example. 
JavaScript, when run in the browser, cannot access the file system for security reasons(Access to the file system is blocked to protect user privacy and security.). However, Node.js extends JavaScript with modules like fs that allow file system operations, using underlying C++ code to interact with the OS.

this (https://github.com/nodejs/node/blob/main/lib/fs.js) is the nodejs code of fs module that handles file system manipulation with the help of C++, so we don't have to write it from scratch. We just import fs module.


üëâüèªCheck the node version 
  >> node -v
üëâüèª Executing Javascript with Node
   Node REPL
  1. Read
  2. Evaluate
  3. Print
  4. Loop
  
üëâüèªBrowser vs Node.js
In the browser, most of the time what you are doing is interacting with the DOM, or other Web Platform APIs like Cookies. In case of Node, we don't have the document, window and all the other objects that are provided by the browser.
And In the browser, we don't have all the nice APIs that Node.js provides through its modules. For example the filesystem access functionality.
With Node.js, you control the environment. But with a browser, you are at the mercy of what the users choose

üëâüèªModules
A module is an encapsulated and reusable chunk of code that has its own context
In Node.js, each file is treated as a separate module
Types of Modules
1. Local modules - Modules that we create in our application
2. Built-in modules - Modules that Node.js ships with out of the box
3. Third party modules - Modules written by other developers that we can use in our application
  
üëâüèªCommonJS
CommonJS is a standard that states how a module should be structured and shared. Node.js adopted CommonJS when it started out and is what you will see in code bases. Each file is treated as a module
Variables, functions, classes, etc. are not accessible to other files by default. 
Explicitly tell the module system which parts of your code should be exported via module.exports or exports
To import code into a file, use the require() function

üëâüèªImport Export Patterns (CommonJS)
‚≠êÔ∏è Pattern 1 : Exporting an Object All at Once
math.js
const add = (a,b)=>{
   return a + b;
}
const subtract = (a,b)=>{
   return a - b;
}
module.exports = {add,subtract}  // short for -> module.exports = {add:add,subtract:subtract}

index.js
const module = require("./math.js")
console.log(module.add(1,2));
console.log(module.subtract(4,7));

‚≠êÔ∏è Pattern 2 : Attaching Properties Directly to module.exports
math.js
module.exports.add = (a,b)=>{ 
   return a + b;
}
module.exports.subtract = (a,b)=>{
   return a - b;
}
// exports.add will work too because if you remember, module wrapper iife has a 'exports' parameter too but using exports.add without using precautions will break the reference to that object and throw error, so it is not used. Full explanation : https://www.youtube.com/watch?v=ghUIlSNRru0&list=PLC3y8-rFHvwh8shCMHFA5kWxD9PaPwxaY&index=15

index.js
const module = require("./math.js")
console.log(module.add(1,2));
console.log(module.subtract(4,7));

‚≠êÔ∏è Pattern 3 : Exporting single instances
math.js
module.exports = (a,b)=>{
  return a + b;
}

index.js
const x = require("./math");
console.log(x(2,3)); // Output : 5

üëâüèªES Modules
At the time Node.js was created, there was no built-in module system in JavaScript
Node.js defaulted to CommonJS as its module system
As of ES2015, JavaScript does have a standardized module system as part of the language itself
That module system is called EcmaScript Modules or ES Modules or ESM for short
ES Modules is the ECMAScript standard for modules. It was introduced with ES2015. Node. js 14 and above support ES Modules.
Instead of module.exports, we use the export keyword. The export can be default or named. We import the exported variables or functions using the import keyword


üëâüèªImport Export Patterns (ECMAScript Module aka ESModule)
‚≠êÔ∏è Pattern 1 : (default Exports)
math.mjs
const add = (a, b) => {  // export default = (a, b) => {  <- this will work too
  return a + b;
};

export default add; // there can only be one export marked as `default`.

index.mjs
import add from ‚Äú./math.mjs";
console.log(add(5, 5));

‚≠êÔ∏è Pattern 2 : (default exports more than one variable/function)
math.mjs
const add = (a, b)=>{
   return a + b;
}
const subtract = (a, b)=>{
   return a - b;
}
export default {add, subtract}

index.mjs
import math from "./math.mjs"; // ‚ùå import {add, subtract} from "./math.mjs"; <- this won't work because this is used for **named exports**, not default exports
console.log(math.add(5, 5));
console.log(math.subtract(5, 5));

‚≠êÔ∏è Pattern 3 : (named exports)
math.mjs
export const add = (a, b)=>{
   return a + b;
}
export const subtract = (a, b)=>{
   return a - b;
}
// we can also include default export with named exports but there should be only one default export

index.mjs
import * as maths from "./math.mjs"; // import {add, subtract} from "./math.mjs"; <- this will work too
console.log(maths.add(5, 5));
console.log(maths.subtract(5, 5));

Note : 
If it is a default export, we can assign any name while importing
If it is a named export, the import name must be the same

üëâüèªModule Scope
Before a module's code is executed, Node.js will wrap it with a function wrapper (IIFE) that provides module scope
This saves us from having to worry about conflicting variables or functions
There is proper encapsulation and reusability is unaffected
(function(){
  // Module code actually lives in here
})
Each loaded module in Node.js is wrapped with an lIFE that provides private
scoping of code

üëâüèªModule Wrapper & Module-scoped variables
Every module in node.js gets wrapped in an lIFE before being loaded
IIFE helps keep top-level variables scoped to the module rather than the global object
The lIFE that wraps every module contains 5 parameters which are pretty important for the functioning of a module
This wrapper function ensures that myVariable is private to myModule.js and doesn't conflict with other module variable if they are of the same name and that myFunction is exposed through the exports object.

myModule.js
(function(message) {
  const superHero = "Batman";
  console.log(message, superHero);
})("Hello");

syntax : 
(function(exports, require, module, __filename, __dirname){ 
    // Module code goes here
})()
Note :  we don't have to wrap the module in iife(the above is just the explanation of how node wrap the module with iife), the node does this automatically. If we try to print these parameters in the iife, we will get undefined because we have overwrote the default wrapper and these have become iife function's local variables. Try to print all these variables without iife and you will get their appropriate values.

1. exports    : A reference to `module.exports`. It's used to export values from a module.
2. require    : A function to import other modules (CommonJS).
3. module     : Represents the current module, with info like `module.exports`, `id`, etc.
4. __filename : The full path to the current file (`index.js` in your case).
5. __dirname  : The full path to the directory containing the current file.

These are automatically injected by Node into every file you run.

üëâüèªModule Caching
When you require() a module in Node.js, that module is loaded only once and then cached.
The first time a module is loaded, Node executes it and stores the result in memory.
Any subsequent require() calls to the same module in any other file/module return the cached version, not a re-executed version.
This is why changes in module state (like changing variables) are reflected in other files that require it afterward ‚Äî because they share the same cached instance.
Example : 

myModule.js : 
let x = 1;
function setValue() {
    x = 2;
}
function getValue() {
    return x;
}

file1.js
const {x,setValue,getValue} = require("./myModule"); // First time `myModule` is required -> it's executed and cached
console.log(x);    // Logs 1 (copied value at export)
setValue();
console.log(getValue()); // Logs 2 because it calls the function that accesses internal `x`
console.log(x); // this x will print the snapshot of the x instead of the actual x which was changed after calling setValue() function, that's why above function getValue() is used to get the updated value.
// ‚ö†Ô∏è x is exported as a value in myModule.js, so its initial value (1) is copied. But setValue and getValue are functions, and they maintain a live reference to the x inside the module.

file2.js
const {x,getValue} = require("./myModule"); 
// `myModule` is NOT re-executed. It is loaded from cache!
// Therefore, `x` inside the module is already set to 2 (by `file1`)
console.log(getValue());
module.exports = {x,setValue,getValue};

index.js
console.log('file1 :-');
require("./file1");	  // Loads file1, which loads `myModule`, sets `x = 2`
console.log('file2 :-');
require("./file2");	  // Loads file2, reuses cached `myModule`, sees `x = 2`

Output : 
1  // x from file1 (copied when imported)
2  // getValue after setValue updated `x`
2  // getValue from file2 uses shared module state


üëâüèªWatch Mode
Watch mode in Node. js allows you to automatically restart your server whenever you make changes to your code, streamlining the development process. This eliminates the need to manually restart the server after every code modification, saving you time and effort. Introduced as an experimental feature in Node
To turn on watch mode, run the file with below command :
>> node --watch index.js


üëâüèªBuilt-in Modules
Node.js includes several built-in modules that provide core functionalities without needing external libraries. These modules are part of the Node.js binary distribution and are essential for various tasks. 
They are also referred to as core modules. It is mandatory to import the module before you can use it
Some common module are : 
1. path
2. events
3. fs
4. stream
5. http

If you are interested in the source code of these build-in modules, they are present in the lib folder of the codebase of nodejs 
https://github.com/nodejs/node/tree/main/lib

üëâüèªPath Module
The path module provides utilities for working with file and directory paths
The path module in Node.js is a built-in module that provides utilities for working with file and directory paths. It offers a variety of functions to manipulate, normalize, and resolve paths, ensuring cross-platform compatibility.
Commonly Used Functions:
path.join(): Joins path segments.
path.resolve(): Resolves a sequence of paths to an absolute path.
path.normalize(): Normalizes a path string.
path.dirname(): Extracts the directory name.
path.basename(): Extracts the file name (with or without extension).
path.extname(): Extracts the file extension.
path.isAbsolute(): Checks if a path is absolute.
path.parse(): Parses a path string into an object.
path.format(): Constructs a path string from an object.
path.sep: Provides platform-specific separator. 

üëâüèªEvents Module
The events module in Node.js is a core module that provides a way to work with events. It allows you to create, emit, and listen for custom events in your application. This module is crucial for building asynchronous and event-driven applications. 
An event is an action or an occurrence that has happened in our application that we can respond to
Using the events module, we can dispatch our own custom events and respond to those custom events in a non-blocking manner

Events Module - Scenario
Let's say you're feeling hungry and head out to Dominos to have pizza
At the counter, you place your order for a pizza
When you place the order, the line cook sees the order on the screen and bakes the pizza for you
Order being placed is the event
Baking a pizza is a response to that event


üëâüèªStreams
https://nodejs.org/api/stream.html
https://www.youtube.com/watch?v=FZOEL5nByYg
A stream is a sequence of data that is being moved from one point to another overtime
Ex: a stream of data over the internet being moved from one computer to another
Ex: a stream of data being transferred from one file to another within the same computer
Work with data in chunks instead of waiting for the entire data to be available at once
Process streams of data in chunks as they arrive instead of waiting for the entire data to be available before processing
Ex: watching a video on YouTube
The data arrives in chunks and you watch in chunks while the rest of the data arrives over time
Ex: transferring file contents from fileA to fileB
The contents arrive in chunks and you transfer in chunks while the remaining contents arrive over time
Prevents unnecessary data downwload and memoery usage

If you're transferring file contents from fileA to fileB, you don't wait for entire fileA content to be saved in temporary memory before moving it into fileB
Instead, the content is transferred in chunks over time which prevents unnecessary memory usage
Stream is infact a built-in node module that inherits from the event emitter class

We rarely use Streams direclty, other modules internally use stream for their functioning

const fs = require("node:fs");

const readableStream = fs.createReadStream("./file1.txt", {  // readable stream to read data in chunks from file1.txt
   encoding: "utf-8"
}); 

const writeableStream = fs.createWriteStream("./file2.txt"); // writable stream to write data in chunks to file2.txt

readableStream.on("data", (chunk) => {
	console.log(chunk) ;
	writeableStream.write(chunk);
})

output : Hello Avinash

streams extend from event emitter class. Event emitter allow us to add listeners to events. above, the readableStream emmits the "data" event on which we can listen and we specify a callback function that should be executed on the data event.
The "data" event is emitted by readable streams (like fs.createReadStream) whenever a chunk of data becomes available to read.

Why Use Stream Instead of fs.readFile/fs.writeFile? (https://www.youtube.com/watch?v=64LJJhT6Ybo)
Because:
> It reads large files in small chunks (not all at once)
> Uses less memory and is non-blocking
> Ideal for large files, streaming video/audio, etc.

The Buffer that Streams use has a default size of 64kb. We can modify it using "highWaterMark" key :
const readableStream = fs.createReadStream("./file1.txt", { 
   encoding: "utf-8",
   highWaterMark:2  // read data in chunks of 2 bytes
});

The Event Emmiter readableStream.on will listen on every 2 byte of chunk so the output when printing chunk in the event will be :
He
ll
o
Av
in
as
h

Types of Streams
1. Readable streams from which data can be read. Ex: Reading from a file as readable stream
2. Writable streams to which we can write data. Ex: Writing to a file as writable stream
3. Duplex streams that are both Readable and Writable. Ex: Sockets as a duplex stream
4. Transform streams that can modify or transform the data as it is written and read Ex: File compression where you can write compressed data and read de-compressed data to and from a file as a transform stream


Chunk Size :
The Buffer that Streams use has a default size of 64kb.
and default chunk size in a readable stream depends on the type of stream you're working with:
1. File Streams (fs.createReadStream)
Default chunk size: 64 KB ‚Üí which is 64 * 1024 = 65536 bytes
This is defined internally by Node as the highWaterMark option.
const readable = fs.createReadStream('file.txt');
console.log(readable._readableState.highWaterMark); // 65536 (64 KB)
You can override it like this:
fs.createReadStream('file.txt', { highWaterMark: 16 * 1024 }); // 16 KB

2. Network Streams (e.g., HTTP)
Default chunk size: 16 KB (16384 bytes)
Example: when reading from a TCP socket or HTTP request/response stream


NOTE : HTTP request is a readable stream and HTTP response is a writeable stream
https://github.com/69JINX/FrontEnd/blob/main/Notes/Notes_Pic/HTTP-request-response-streams.png
HTTP Streaming when querying large data from a database. Providing data in chunks to frontend : https://www.loginradius.com/blog/engineering/guest-post/http-streaming-with-nodejs-and-fetch-api

üëâüèªPipes
https://www.youtube.com/watch?v=5Mosdd3jwgU
It is used for large data, real-time processing, or when chaining stream operations.
It connects a readable stream to a writable stream, allowing data to flow automatically between them ‚Äî chunk by chunk ‚Äî without manually handling 'data' and 'end' events. .pipe() is a method available on readable streams in Node.js (we cannot use .pipe() method on writable streams).
Benefits of .pipe()
- Simplicity -> Shorter and cleaner code
- Backpressure -> Automatically manages flow so memory doesn‚Äôt overflow
- Error handling -> Can chain .on('error') listeners
- Streaming efficiency -> Great for large data (files, HTTP responses, etc.)

Pipe Chaining :
It returns destination streams which enable chaining, but the conditions is that stream has to be readable, duplex or a transform stream
Pipe chaining in Node.js refers to the process of connecting multiple streams together to create a data processing pipeline. This is achieved using the pipe() method, which is available on readable streams. The pipe() method takes a writable stream as an argument, and it automatically forwards data from the readable stream to the writable stream.
When chaining pipes, the output of one stream becomes the input of the next stream. This allows for complex data transformations to be performed in a sequential manner. 
For example, consider the following scenario: reading data from a file, compressing it using gzip, and then writing the compressed data to another file. This can be achieved using pipe chaining as follows:

Pipe chaining offers several benefits:
Modularity: It allows you to break down complex data processing tasks into smaller, manageable components.
Readability: It makes code more readable and easier to understand by visually representing the flow of data.
Efficiency: It avoids loading large amounts of data into memory at once, improving performance and memory usage.
Flexibility: It allows you to easily combine different types of streams to perform various data transformations.

syntax : readable.pipe(transform1).pipe(transform2).pipe(writable);

const fs = require('fs');
const zlib = require('zlib');

fs.createReadStream('input.txt')              // readable
  .pipe(zlib.createGzip())                    // transform (compress)
  .pipe(fs.createWriteStream('output.txt.gz')) // writable
Note : What is this output.txt.gz file here ?
This is a Gzipped (compressed) version of your original file. This Compresses data using the Gzip algorithm. The compressed version of input.txt, smaller and optimized for storage/transmission
Use Cases:
Reduce file size to save disk space or bandwidth.
Send compressed files over HTTP (faster delivery).
Store backups efficiently.
Package logs or data for transmission.
It's not a text file anymore ‚Äî it's a binary compressed file that needs to be decompressed to view its original content.
To view the content of the gz or de-compress it :
-> for windows - use winrar/7zip
-> for Linux/macOS terminal - gunzip output.txt.gz (gunzip is preinstalled in linux/mac)
-> In Node.js (to reverse the operation): 
   fs.createReadStream('output.txt.gz')
     .pipe(zlib.createGunzip())
     .pipe(fs.createWriteStream('original.txt'));

How .pipe() handle backpressure :
Backpressure occurs when a readable stream produces data faster than a writable stream can consume it. Imagine a scenarion where readable stream is 4mb/s and writable stream is 3mb/s. This incosistency of speed in both side will lead to potential memory issues and overwhelm the writable stream which can't handle this much data because of missmatch of the data speed. 
Here's how pipe() fix this -
- Automatic Flow Control: The pipe() method automatically manages the flow of data between a readable and a writable stream. It pauses the readable stream when the writable stream's buffer is full, preventing data overload.
- 'drain' event: When the writable stream is ready to receive more data, it emits a 'drain' event. The pipe() method listens for this event and resumes the readable stream, ensuring a smooth data flow.
- Simplified Backpressure Management: By using pipe(), you don't have to manually manage pausing and resuming streams. Node.js handles this automatically, making your code cleaner and less error-prone.
- Error Handling: pipe() also manages errors. If an error occurs in either the readable or writable stream, it propagates the error to the other stream, preventing data loss.

const readableStream = fs.createReadStream("./file1.txt", { 
   encoding: "utf-8"
}); 
const writeableStream = fs.createWriteStream("./file2.txt");
readableStream.pipe(writeableStream);

using .pipe() eliminates the need to manually use these EventEmitter (but you can still use these while still utilizing .pipe()) : 
- .on('data', handler)
- .on('end', handler)
- .on('error', handler)

Pipe and Streams 
Streams and pipes are closely related in Node.js.
- Streams represent a sequence of data that can be read or written over time. They are a fundamental concept for handling data efficiently, especially when dealing with large files or network communication. Node.js offers various stream types, including readable, writable, and transform streams.
- Pipes provide a mechanism to connect readable streams to writable streams. The pipe() method takes a readable stream and redirects its output to a writable stream. This allows for a streamlined data flow, where data chunks are automatically passed from one stream to another as they become available.

üëâüèªBuffers

Ex: Imaginge a scenario of rollar coaster with 30 Seating capacity
Scenario 1
100 - People arrival
30 - People accommodated
70 - People in queue (waiting)
Scenario 2
1 - Person arrives (waiting)
Wait for at least 10

So the Buffer is the area where people wait 
Node.js cannot control the pace at which data arrives in the stream. It can only decide when is the right time to send the data for processing. 
If there is data already processed or too little data to process, Node puts the arriving data in a buffer
It is an intentionally small area that Node maintains in the runtime to process a stream of

Ex: streaming a video online
If your internet connection is fast enough, the speed of the stream will be fast enough to instantly fill up the buffer and send it out for processing. That will repeat till the stream is finished
If your connection is slow, after processing the first chunk of data that arrived, the video player will display a loading spinner which indicates it is waiting for more data to arrive
Once the buffer is filled up and the data is processed, the video player shows the video While the video is playing, more data will continue to arrive and wait in the buffer
In Node.js, a Buffer is a built-in class used to handle binary data directly in memory. It‚Äôs particularly useful when dealing with:
->File system operations (e.g., reading images, PDFs, etc.)
->Network protocols (e.g., TCP streams, HTTP responses)
->Binary streams (e.g., audio, video, or encrypted content)
Think of a Buffer as a raw storage area in memory ‚Äî similar to an array of bytes.

JavaScript was originally created to work in the browser and mainly with strings and objects, not binary data. But Node.js is used on the server, where low-level operations like file I/O, TCP sockets, and cryptography are common ‚Äî all of which involve binary data.
So Buffer fills that gap by letting JavaScript work with binary data efficiently.

üîß Common Use Cases
1. File system reading -> Reading a .jpg or .zip file using fs.readFile
2. Networking -> Processing TCP or WebSocket data chunks
3. Encoding conversions -> Converting between UTF-8, ASCII, Base64, etc.
4. Cryptography -> Hashing or encrypting raw bytes using crypto module
5. Streaming -> Handling chunks of audio/video data

Nodejs internally uses Buffers whenever required

Ex: 
const buffer = new Buffer.from("Avinash"); // the second argument utf-8 is optional becuase it is already utf-8 by-default new Buffer.from("Avinash","utf-8");
console.log('buffer -> ',buffer);
console.log('toJSON -> ',buffer.toJSON());
console.log('toString -> ',buffer.toString());
buffer.write("Ram");
console.log('toString -> ',buffer.toString());
output : 
buffer ->  <Buffer 41 76 69 6e 61 73 68> // raw binary data of each character that is displayed in Hexadecimal. eg 41 = 101001
toJSON ->  {
  type: 'Buffer',
  data: [
     65, 118, 105, // These are the ASCII(or UTF-8) representation of each character in "Avinash" as a array 
    110,  97, 115,
    104
  ]
}
toString ->  Avinash
toString ->  Ramnash

Explanation of above code and a Simple program to show the use of Buffer in real case scenario :
https://github.com/69JINX/FrontEnd/blob/main/Notes/Notes_Pic/Buffer.png

Character Encodings : (Unicode, ASCII, UTF-8 etc) : https://www.youtube.com/watch?v=ut74oHojxqo

Asynchronous JavaScript
JavaScript is a synchronous, blocking, single-threaded language

Synchronous
If we have two functions which log messages to the console, code executes top down, with only one line executing at any given time

Blocking
No matter how long a previous process takes, the subsequent processes won't kick off until the former is completed
Web app runs in a browser and it executes an intensive chunk of code without returning control to the browser, the browser can appear to be frozen

Single-threaded
A thread is simply a process that your javascript program can use to run a task
Each thread can only do one task at a time. JavaScript has just the one thread called the main thread for executing any code

function A() {
  console. log('A")
}

function B() {
  console. log('B")
}

A()
B()

-> Logs A and then B


Problem with synchronous, blocking, single-threaded model of JavaScript

let response = fetchDataFromDB('endpoint')
displayDataFromDB(response)

fetchDataFromDB('endpoint') could take 1 second or even more. During that time, we can‚Äôt run any further code
JavaScript, if it simply proceeds to the next line without waiting, we have an error because data is not what we expect it to be

Just JavaScript is not enough
We need new pieces which are outside of JavaScript to help us write asynchronous code For front-end, this is where web browsers come into play. 
For back-end, this is where Node.js comes into play
Web browsers and Node.js define functions and APIs that allow us to register functions that should not be executed synchronously, and should instead be invoked asynchronously when some kind of event occurs

For example, that could be the passage of time ( setTimeout or setinterval), the user's
interaction with the mouse (addEventListener), data being read from a file system or the
arrival of data over the network (callbacks, Promises, async-await)

You can let your code do several things at the same time without stopping or blocking your main thread

JavaScript is a synchronous, blocking, single-threaded language, This nature however is not beneficial for writing apps. We want non-blocking asynchronous behaviour which is made possible by a browser for FE and Node.js for backend
This style of programming where we don't block the main JavaScript thread is fundamental to Node.js and is at the heart of how some of the built-in module code is written

fs Module
The file system (fs) module allows you to work with the file system on your computer
Ex :
file.txt
Hello Avinash
index.js
const fs = require("fs");

const file_Buffer = fs.readFileSync("./file.txt")
const file_Content = fs.readFileSync("./file.txt","utf-8")
console.log(file_Buffer);
console.log(file_Content);

output :
<Buffer 48 65 6c 6c 6f 20 41 76 69 6e 61 73 68 0a>
Hello Avinash

Sync/Async file read 
index.js
const fs = require("fs");

console.log("First");
const fileContent = fs.readFileSync("./file.txt","utf-8") // Synchronous file read
console.log(fileContent);

console.log("Second");

fs.readFile("./file.txt","utf-8",(error,data)=>{ // Asynchronous file read. This patter is using callbacks where first argument is the error is called "error first callback pattern"
   if(error){
      console.log(error,"Error reading file");
    }
    else{
     console.log(data);
    }
});

console.log("Third");

output :
First
Hello Avinash

Second
Third
Hello Avinash

Becuase of this asynchronous file read, the app will not freeze when multiple users interact with the application

Create files
index.js
fs.writeFileSync("./greet1.txt","Hello Avinash");
fs.writeFile("./greet2.txt","Hello again Avinash",(err)=>{  
  if(err) return console.log(err);
  console.log("Content has been written in the greet2.txt file");
});

The Above write function will overwrite the content if the file already exist. To append to the existing content, add a flag in the object :
fs.writeFile("./greet2.txt","Hello again Avinash",{flag:'a'},(err)=>{  

fs Promise Module
The fs/promises API provides asynchronous file system methods that return promises.
The promise APIs use the underlying Node.js threadpool to perform file system operations off the event loop thread. These operations are not synchronized or threadsafe. Care must be taken when performing multiple concurrent modifications on the same file or data corruption may occur.

const fs = require("fs");

//handling with then-catch
fs.promises.readFile("./file.txt","utf-8")
.then((data)=>{
 console.log(data);
}).0
.catch((err)=>{
 console.log(err);
})

//handling with async-await
async function readFile(){
  try{
    const data = await fs.promises.readFile("./file.txt","utf-8")
    console.log(data);
  }
  catch(err){
    console.log(err);  
  }
}
readFile();

Tip : this works too :
const fs = require("fs/promises");
fs.readFile("./file.txt","utf-8") // will still return a promise

Note : The callback based version of the fs module api are preferable over the use of promise api when maximum performance is required both in terms of execution time and memory allocation


üëâüèª HTTP Module

How the web works
Computers connected to the internet are called clients and servers

Clients :  are internet-connected devices such as computers or mobile phones along with web-accessing software available on those devices such as a web browser
Servers : on the other hand are computers that store web pages, sites, or apps

When you type a url in the browser, the client device request access to the webpage. A copy of the webpage is downloaded from the server and send as a response to the client to displayed in the browser. This Model is popularly called Client-Server Model
Now we understood, the data transfer between client & server, but in what format is that data. What if the request send by the client cannot be understood by the server, or what if the response send by the server cannot be understod by the client, well  that's where the HTTP comes into the picture.
-> HTTP
Hypertext Transfer Protocol
A protocol that defines a format for clients and servers to speak to each other 
The client sends an HTTP request and the server responds with an HTTP response

-> HTTP and Node
We can create a web server using Node.js
Node.js has access to operating system functionality like networking
Node has an event loop to run tasks asynchronously and is perfect for creating
web servers that can simultaneously handle large volumes of requests
The node server we create should still respect the HTTP format
The HTTP module allows creation of web servers that can transfer data over HTTP
