üëâüèªNPM
1. Is is the world's largest software library (registry)
2. It is a software package manager

npm is a Software Library
A book library contains books written by various authors
npm is a library or a registry which contains code packages written by various developers
It is a large public database of JavaScript code that developers from all over the world can use to share and borrow code
If you author a ‚Äúcode package‚Äù, you can publish it to the npm registry for others to use
If you come across a code package that is authored by someone else and solves the problem you have at hand, you can borrow that code without having to reinvent the wheel

üëâüèªpackage.json
What?
package.json is npm's configuration file
It is a json file that typically lives in the root directory of your package and holds various metadata relevant to the package
Why?
package.json is the central place to configure and describe how to interact with and run your package
It is primarily used by the npm CLI

Node Playlist : https://www.youtube.com/watch?v=4KC-TxNBWsE

üëâüèªVersioning 
Versioning of node packages
ex: package.json
"dependencies": {
  "react-icons": "^5.3.0",
 }
 
Semantic Versioning
SemVer - is one of the most widely adopted versioning systems
A simple set of rules and requirements that dictate how version numbers are assigned and incremented
It is crucial to keep a semantic and historical track of changes
Version numbers and the way they change convey meaning about the underlying code and what has been modified from one version to the next
syntax : X.Y.Z (where x stands for Major version, Y stands for Minor version and Z stands for a Patch)

Versioning Rules :

> When you fix a bug and the code stays backwards-compatible you increment the patch version.
For example 1.1.1 to 1.1.2

> When you add new functionality but the code still stays backwards-compatible,
you increment the minor version
You also reset the patch version to zero
For example 1.1.1 to 1.2.0

> When you make changes and the code is no more backwards compatible, you
increment the major version
You have to reset the minor and patch version to zero
For example 1.1.1 to 2.0.0.

Semantic versioning always starts with 0.1.0
0.Y.Z (a major version of zero) is used for initial development
When the code is production-ready, you increment to version 1.0.0
Even the simplest of changes has to be done with an increase in the version number

üëâüèªScripts
An npm script is a convenient way to bundle common commands for use in a project
They are typically entered in the command line in order to do something with the application
npm scripts are stored in a project's package.json file, giving access to everyone who has access to the codebase
They also ensure that everyone is using the same command with the same options
Common use cases for npm scripts include building your project, starting a development server, compiling CSS, linting, minifying etc
npm scripts are executed using the command npm run <SCRIPT_NAME>

ex: package.json
"scripts": {
  "start": "node index.js",
 }
>> npm run start
or
>> npm start

npm test, npm start, npm restart, and npm stop are all aliases for npm run xxx.
For all other scripts you define, you need to use the npm run xxx syntax. (See the docs at https://docs.npmjs.com/cli/commands/npm-run for more information.)

üëâüèªBuilding CLI Tools
CLI stands for Command Line Interface. A program that you can run from the terminal
Ex: npm and git

Create a basic CLI tool using node and npm
Pass options to the CLI
Add interactivity to the CLI

üëâüèªCluster Module
Node is single threaded
No matter how many cores you have, node only uses a single core of your CPU
This is fine for I/O operations but if the code has long running and CPU intensive operations, your application might struggle from a performance point of view
The cluster module enables the creation of child processes (also called workers) that run simultaneously
> The Cluster module allows you to leverage multi-core CPUs by creating multiple worker processes that share a single server port. This enables your Node.js application to handle more concurrent requests and improve performance for CPU-intensive tasks.
> The Cluster module creates child processes (workers), each running independently and in its own event loop. A master process manages these workers, distributing incoming connections among them.
> It is essential for scaling Node.js applications that need to handle a high volume of concurrent requests or perform significant CPU-bound computations, allowing them to utilize all available CPU cores effectively.

ex (no_cluster.js) : 
const express = require('express');
const app = express();

app.get('/',(req,res)=>{
  res.end("Home Page");
});

app.get('/slow-page',(req,res)=>{
  for(let i = 0; i<6000000000;i++){} // Simulate CPU work 
  res.end("Slow Page");
});

app.listen(4000,()=>{
  console.log('Server is running at port 4000');
})

If you try to access both pages. In the network tab on the Time column (usually 6th column), you can see the home page took almost 4ms and fetched very quickly, on the other hand the /slow-page page took 6.32s (that is way slower then home page because we have a long running for-loop in the controller)

> Lets observe a little scenario. 
Try to reload (/slow-page) on a tab. While the slow-page is being load, reload the the home page on a new tab right after reloading the /slow-page. Because of the /slow-page, the home page will be postpond and will wait for the /slow-page to finish loading and it will also take around 5s.
Why is this happening ?
Well that is because the single thread of nodejs is blocked with the for-loop and server won't be able to respond to any new request. /slow-page is basically blocking the home page. If 1 user hits /slow-page, Everyone else ‚Äî even those trying to access, will get stuck waiting.
The solutions for this scenarion are :
Use non-blocking code or offload heavy tasks using:
> Async patterns: e.g., setTimeout, Promise, async/await with non-blocking I/O
> Worker threads (built-in in Node.js): offload CPU-intensive tasks
> Child processes (for heavy computation)
> Cluster mode (multiple Node.js processes via PM2 or native cluster module)

We will be using Cluster Module to solve this issue.
How cluster module works in nodejs ?
By default, index.js runs in a single process ‚Äî and this process does handle all logic: routing, file access, database operations, etc.
No master/worker separation happens unless you explicitly implement clustering using Node.js‚Äôs cluster module.
Now, if you use the cluster module :
> When we run index.js (node index.js) in the terminal, the file is treated as a "Cluster Master" and this master is encharge of spawning new workers which runs an instance of our node application.
> It is very important to note that the Master is only in charge of the workers (starting, stopping, restarting them if they crash etc.) but doesn not execute the application code itself (like handling HTTP requests or running your Express app). It is not incharge of handling incoming request, reading files etc. That is up to indivisual worker instance.
> Each worker gets its own event loop, memory, and V8 instance and Each worker handles its own incoming requests(like HTTP requests). and doing so we are able to share the workload across different instances without having to block incoming requests

ex : 
const cluter = require("cluster");

if(cluter.isMaster) console.log(`Master process ${process.pid} is running`);
else console.log(`Worker ${process.pid} started`);

output :
Master process 66249 is running

Creating new workers with cluster.fork()
Now let's focus on what code to run as Master vs Worker. As master we need to create new workers. To do that, we use fork() method on the cluster object. Create multiple workers by calling the fork method on the cluster multiple times.
cluster.fork(); // Worker 1
cluster.fork(); // Worker 2

once the workers are created, no incoming requests will be handle by master cluster. every process will be handle by cluster workers. 

ex (creating workers) : 
const express = require("express");
const app = express();
const cluster = require("cluster");

if(cluster.isMaster) {
  console.log(`Master process ${process.pid} is running`);
  cluster.fork();
  cluster.fork();
}
else {
  console.log(`Worker ${process.pid} started`);
  app.listen(4000,()=>{
    console.log('Server is running at port 4000');
  })
}

output :
Master process 67409 is running
Worker 67417 started
Worker 67416 started
Server is running at port 4000
Server is running at port 4000

Flow of the code :
1. cluster.isMaster is true for the main process: 
 Logs:
 Master process 67409 is running
 Then it forks two worker processes:
 cluster.fork();
 cluster.fork();
2. Each worker process re-runs the entire file: (in this case, 2 clusters = 2 times file running)
 BUT now cluster.isMaster === false, so both workers start their own Express app on port 4000 so now we have 2 servers to handle more requests and traffic.
3.Each worker runs independently and listens on the same port ‚Äî this is allowed because Node.js Cluster lets the kernel handle load balancing between them.

How the worker is choosen to process a specific task ?
The master process listens on the port behind the scenes.
When a request comes in, the OS kernel (like Linux) or Node's cluster module:
 > Picks a worker (usually via round-robin strategy),
 > And passes the connection to that worker process using IPC (inter-process communication).
So workers don‚Äôt compete ‚Äî the master dispatches the requests.

Let‚Äôs revisit the previous scenario where accessing the /slow-page endpoint caused the entire application to become unresponsive ‚Äî blocking access to other routes like the home page ‚Äî until the /slow-page request completed.

With clustering enabled and multiple worker processes spawned, this bottleneck is eliminated. If one worker is occupied handling a high-computation route like /slow-page, incoming requests (such as to /) can now be delegated to an available worker. As a result, the home page remains responsive and loads almost instantly (within 2‚Äì3 ms), even while the /slow-page is still processing (which may still take ~4.5 seconds due to CPU-intensive operations).

In essence, clustering leverages multi-core systems by distributing load across separate worker processes, ensuring that a single blocking request does not degrade the responsiveness of the entire application.

example 2 : 
const express = require("express");
const cluster = require("cluster");
const os = require("os");


if(cluster.isPrimary) {
  console.log(`Master process ${process.pid} is running`);

  for(let i=0;i<os.cpus().length;i++) // creating worker as there are no. of cpus cores
  {
    cluster.fork();   
  }
}
else {
  const app = express();
  
  app.get('/',(req,res)=>{
    res.end(`Worker ${process.pid} completed you request`);
  });

  console.log(`Worker ${process.pid} started`);
  
  app.listen(4000,()=>{
    console.log('Server is running at port 4000');
  })
}
output :
Master process 14702 is running
Worker 14709 started
Worker 14710 started
Server is running at port 4000
Worker 14711 started
Server is running at port 4000
Worker 14717 started
Server is running at port 4000
Server is running at port 4000
Worker 14719 started
Server is running at port 4000
Worker 14727 started
Server is running at port 4000

Try to access localhost:4000 on different browsers, you will see different workers id as the request was solved by different workers.
browser 1 : Worker 14709 completed you request
browser 2 : Worker 14710 completed you request

isMaster vs isPrimary :
cluster.isMaster is an older, deprecated property.
cluster.isPrimary is the current, recommended equivalent.
For new development or when updating existing code, cluster.isPrimary should be used to ensure compatibility with recent Node.js versions and best practices. If supporting older Node.js versions is necessary, a check like if (cluster.isPrimary || cluster.isMaster) can be employed for broader compatibility.

Note : It is very important that you create 2 worker threads minimum. If you create only 1, it is the same as no-cluster scenario. The master will not handle any incoming request, resulting in only 1 node instance responsible for all requests.

Why shouldn't we simply create a large number of workers using cluster.fork()?
We should only create as many workers as there are CPU cores on the machine the app is running
If you create more workers than there are logical cores on the computer it can cause an overhead as the system will have to schedule all the created workers with fewer number of cores

How to see number of cores of your system ?
>> require("os").cpus().length;

You can use packages to run your application as cluster which will also decide the best number of workers to create for your machine. One of those npm packages is "PM2"

PM2 (Process Manager 2)
PM2, short for Process Manager 2, is a popular production process manager for Node.js applications. It helps developers manage, monitor, and keep Node.js applications running continuously, especially in a production environment.

>> npm install pm2 -g
PM2 should be installed globally so it can be accessible from anywhere on your system using (CLI) Global installation ensures that pm2 commands like start, stop, restart, and logs are available system-wide, simplifying application management, especially in production environments.  
for more info, visit : http://pm2.io/
PM2 quick start : https://pm2.keymetrics.io/docs/usage/quick-start/


Key Features and Functions:
1. Process Management:
	PM2 allows you to start, stop, restart, and manage your Node.js applications, ensuring they remain online and available. 
2. Process Monitoring:
	It provides features to monitor application status, logs, and resource usage (CPU and memory). 
3. Self-Recovery:
	PM2 automatically restarts applications if they crash or encounter errors, minimizing downtime. 
4. Clustering and Load Balancing:
	PM2 can distribute traffic across multiple instances of an application, enabling scalability and improving performance. 
5. Hot Reloading:
	PM2 can reload applications without any downtime when code changes are detected. 
6. Deployment System:
	PM2 offers a deployment system that can streamline the process of deploying applications to different environments. 
7. Configuration Files:
	You can create configuration files (Ecosystem files) to manage multiple applications and their settings. 
8. Keymetrics Integration:
	PM2 integrates with Keymetrics, a monitoring platform, for more advanced application insights. 
9. Cross-Platform Compatibility:
	PM2 works on Linux, macOS, and Windows operating systems. 
10. Supports various languages:
	While primarily known for Node.js, PM2 can also manage other applications written in languages like Python and Ruby, and even binaries in your PATH. 

How it Works:
1. Installation:
	PM2 is installed globally as an npm package. 
2. Starting Applications:
	You use the pm2 start app.js command to start your application, which then runs as a daemon (a background process). 
3. Background Daemon:
	PM2 runs in the background, continuously monitoring and managing the application. 
4. Commands for Management:
	PM2 provides commands like pm2 list (to see running processes), pm2 stop app.js (to stop), pm2 restart app.js (to restart), and pm2 delete app.js (to remove).

Running a app using PM2
when running our app using pm2, we don't have to import the cluster module. The pm2 automatically create workers.
to run our index.js using pm2 
>> pm2 start no_cluster.js -i 0
To enable the cluster mode, just pass the -i option. the zero indicates we want pm2 to figure out optimum number of workers to create, if you specify the number eg. 2, pm2 will create only 2 workers
Once the pm2 has registered a process with a certain cpu core, you can restart/stop your app with only id of the process instead of using the full file name.

If you want to stop a certain process/worker, use the id of the process/worker
>> pm2 stop [id]
or stop all process
>> pm2 stop all
Same command for the "start" command

If you want to unregistered some process from the cpu core, you can use 
>> pm2 delete [id]  
or delete unregister all process with
>> pm2 delete all

Logs :
All console logs or errors will not be printed directly into the terminal once your app has been started using pm2 start. 
Instead, to see all console logs of your app in realtime, use the below command.
>> pm2 logs
It will also show the worker number associated with the file name (main index file, if there are multipe files) which was used to print that console.

Note : we should spawn only 1 app per core. If you try to run processes than there are no. of cpu cores, the Context switching overhead might increase, Performance may degrade, especially under heavy CPU load, CPU-intensive routes in one worker may still block requests for that worker‚Äôs queue.

Running Frontend nextjs app with pm2 (usually on production environment like EC2) :

pm2 start npm --name My-Frontend_production -- start (in production build for 'npm run start'/'npm start')
pm2 start npm --name My-Frontend_development -- run dev (in development mode for 'npm run dev')
(if you don't want to give any name to the process, it can be as simple as :
>> pm2 start npm -- start (production)
or
>> pm2 start npm -- run dev  (development)

Breakdown :
How 'npm start' converted to this long pm2 code from above
1. pm2 start ‚Üí tells PM2 to start a process
2. npm ‚Üí the executable to run (like server.js for backend)
3. --name next-app ‚Üí PM2 name for managing the app
4. -- ‚Üí very important ‚Äî separates PM2‚Äôs own arguments from the arguments you want to pass to the script
5. start ‚Üí argument passed to npm, making it:

Every item after -- is treated as a space-separated argument to the script (npm/server.js), not to PM2 itself.
So this: 
>> pm2 start npm --name next-app -- start
is like manually running:
>> npm start
And PM2 just manages the process.

For example:
>> pm2 start npm --name next-app -- run start -- --port 3001
This would translate to:
>> npm run start -- --port 3001
(npm run dev ignores args like port after the script name unless they come after a --, so port 3001 will be ignored if we didn't use double dash before --port 3001. This is npm's rule, not PM2's)

‚úÖ General Syntax:
- PM2 args (like --name) must go before the first --
- Everything after the first -- goes to the script (like npm start)
so this won't work :
>> pm2 start npm -- run dev --name My-Frontend
but this will :
>> pm2 start npm --name My-Frontend -- run dev

üß† Golden Rule for PM2 Command Arguments:
üîπ All arguments for PM2 go before the first --
üîπ All arguments for your script (e.g., npm, next, Node.js) go after the first --
üîπ Environment variables can go either inline+ or in a config file

Syntax:
pm2 start <command> [PM2 options] -- [script args]

if including env variables in the command, they should come before the pm2 command
>> PORT=3001 pm2 start server.js

Note : It is recommended to restart your already running frontend instance instead of deleting it and creating new with above script. 
The above script is usually used to start a nextjs app for the first time with pm2
>> npm run build (create a new build after code updation)
>> pm2 restart 0 (use the number/id on whatever id the frontend process is running ["pm2 list" to see all running processes and their ids])
(incase of scenario where frontend is not reflecting changed even after restarting, then it is adviced to delete the already running process (with 'pm2 delete pid' and start a new one)

‚úÖ Alternative with ecosystem file (recommended for production):
Create ecosystem.config.js:
module.exports = {
  apps: [
    {
      name: "next-app",
      script: "npm",
      args: "start",
      env: {
        NODE_ENV: "production",
        PORT: 3000
      }
    }
  ]
};
Start it with:
pm2 start ecosystem.config.js

NODE_ENV : NODE_ENV is an environment variable in Node.js that specifies the environment in which the application is running, typically either "development", "production", or "test". It allows applications to behave differently based on the environment they are in, enabling configurations, optimizations, and behaviors specific to each environment.

üëâüèªWorker Thread

let's revisit some facts about js and nodejs :

How JS/Nodejs is single threaded and can still handle asynchronous tasks ?
JavaScript is fundamentally a single-threaded language. This means it has one call stack and one memory heap, and it executes code sequentially, one instruction at a time. It must complete the execution of a piece of code before moving on to the next.

However, JavaScript achieves asynchronous behavior and appears to handle multiple tasks concurrently through mechanisms like the event loop and Web APIs (in browsers) or the libuv library (in Node.js). This allows for non-blocking operations, such as handling network requests or I/O operations, without freezing the main execution thread.

While JavaScript itself is single-threaded, environments like browsers and Node.js provide features like Web Workers (in browsers) that allow for true multi-threading by running scripts in separate background threads, enabling the execution of computationally intensive tasks without impacting the main thread's responsiveness.

in Nodejs, the event loop continuously checks for completed async operations and runs their callbacks without blocking the main thread. When an I/O task (like file or network access) is triggered, it's offloaded to the OS or libuv‚Äôs thread pool. Once done, the callback is queued and executed when the call stack is clear.

libuv enables non-blocking I/O by handling heavy I/O work in background threads, allowing the main thread to stay responsive. This design lets Node.js manage many concurrent connections efficiently. However, CPU-intensive tasks can still block the event loop. For such cases, Node.js provides Worker Threads‚Äîa way to run CPU-heavy JavaScript code in parallel threads, enabling true multithreading without affecting the main thread‚Äôs performance.

also revisit the Thread Pool topic from previous

Network I/O operations on node.js runs on the main thread. it's handled by the main thread using non-blocking I/O, which is what makes Node.js fast and efficient.

Yes, node.js spawns four threads(as we previously read in Thread pool topic) in addition to the main thread but none of them are used for network I/O such as database operations. The threads are:

1. DNS resolver (since some OSes don‚Äôt support async DNS natively)
2. File system API (because this is messy to do asynchronously cross-platform, reading/writing files)
3. Crypto (Because this uses the CPU, like password hashing)
4. Zlib (zlib operations like zipping files)

‚ùì Why is Node.js fast if it's single-threaded?
Most I/O operations (like calling a database) don‚Äôt use the CPU much. They mostly involve waiting for a response from another machine. Node.js takes advantage of this by not blocking the main thread while waiting. Instead:

When a request (like a DB query) is sent, Node.js moves on to handle other tasks.

When the response comes back, Node.js runs the callback function that handles the result.

This is called non-blocking I/O, and it allows Node.js to handle many requests at the same time, even with just one main thread.

üßµ What if you want to use multiple CPU cores?
If you have CPU-heavy tasks (like image processing or math), they can block the main thread. In such cases, you can:

>> Use worker_threads to run that logic in separate threads.
>> Use the cluster module or a process manager like PM2 to run multiple Node.js processes on different CPU cores.

Node.js manages thread pools through two primary mechanisms:
1. Libuv's Thread Pool:
> Node.js leverages the libuv library, which provides a default thread pool of four threads.
> This pool is automatically used for offloading I/O-bound and CPU-intensive operations that would otherwise block the main event loop. Examples include file system operations (e.g., fs.readFile), DNS lookups, and cryptographic functions (e.g., crypto.pbkdf2).
> The size of this thread pool can be adjusted using the UV_THREADPOOL_SIZE environment variable, though increasing it beyond the number of logical CPU cores may not always yield performance benefits.
2. Worker Threads Module:
> Introduced in Node.js 10.5.0, the worker_threads module allows developers to create and manage their own threads explicitly.
> These worker threads run in separate execution contexts, similar to child processes, but within the same Node.js process. This enables parallel execution of CPU-bound tasks without blocking the main event loop, while still allowing for efficient data sharing through mechanisms like SharedArrayBuffer and MessagePort.
>These are separate, independent threads of execution, each with its own memory space and V8 instance. They are useful for CPU-intensive tasks that would otherwise block the main thread. 
> Developers are responsible for managing the lifecycle of these worker threads, including creation, communication, and termination.

Worker Thread:

Node.js Worker Threads, introduced in Node.js 10.5.0 and stable since Node.js 12, provide a mechanism for running JavaScript code in parallel on separate threads within a single Node.js process. This addresses the traditional single-threaded nature of Node.js and its event loop, which can be blocked by CPU-intensive tasks.
Why Worker Threads?
1. Parallelism for CPU-bound tasks:
Node.js excels at I/O-bound operations due to its non-blocking event loop. However, CPU-intensive tasks (e.g., complex calculations, image processing, data compression) can block the main thread, leading to application unresponsiveness. Worker threads allow offloading these tasks to separate threads, preventing event loop blockage and improving overall performance.
2. Improved responsiveness:
By delegating heavy computations to workers, the main thread remains free to handle incoming requests, user interactions, and other I/O operations, ensuring a responsive application.

Key Concepts:
1. worker_threads module:
The built-in module providing the API for creating and managing worker threads.
2. Worker class:
Used to create new worker threads. It takes a path to a JavaScript file or a textual script as an argument, which will be executed in the worker thread.
3. isMainThread:
A boolean property within the worker_threads module that indicates whether the current code is running in the main thread or a worker thread. This allows using the same script for both main and worker logic.
4. parentPort:
An object available within a worker thread, representing the communication channel to its parent thread.
5. Message Passing:
Communication between the main thread and worker threads occurs via message passing using worker.postMessage() (from main to worker) and parentPort.postMessage() (from worker to main). Data passed between threads is copied, not shared directly, except for ArrayBuffer instances which can be transferred, and SharedArrayBuffer instances which can be shared.
6. Event Handling:
Worker threads emit events like 'message' (for received messages), 'error' (for uncaught errors within the worker), and 'exit' (when the worker thread terminates).

How they work:
Each worker thread runs in its own isolated V8 instance and has its own event loop. This isolation prevents a crashing worker from affecting the main application. While workers share some resources with the main process (like network sockets), they operate independently, enabling true parallelism for JavaScript execution. 

Use Cases:
Heavy data processing and transformations, Image and video manipulation, Cryptographic operations, Complex mathematical computations, and Machine learning inference.

index.js
const express = require('express');
const app = express();
const { Worker } = require('worker_threads');

app.get('/',(req,res)=>{
  res.end("Home Page");
});

app.get('/slow-page',(req,res)=>{
  const worker = new Worker('./worker-thread.js'); // creating new worker thread
  worker.on('message',(j)=>{  // listening to messages from worker thread using message event
    res.end("Slow Page " + j);
  });
});

app.listen(4000,()=>{
  console.log('Server is running at port 4000');
})

worker-thread.js
const { parentPort } = require('worker_threads');

let j = 0;

for(let i = 0; i<6000000000;i++) j++;

parentPort.postMessage(j); // sending message to main (parent) thread


Number of Worker Threads :
1. CPU-Bound Tasks: For tasks that heavily utilize the CPU (e.g., complex calculations, image processing, video compression), a common recommendation is to create a number of worker threads equal to or slightly less than the number of available CPU cores. This allows for parallel execution without excessive context switching overhead. For example, if your machine has 8 cores, you might create 7 worker threads

2. I/O-Bound Tasks:
For tasks that primarily involve waiting for external resources (e.g., database queries, network requests), adding more worker threads beyond the number of CPU cores may not significantly improve performance as the bottleneck lies in the external system. Node.js's asynchronous I/O model already handles these efficiently.

Key Considerations:
1. Overhead: Too many threads can hurt performance due to memory usage and context switching.
2. Worker Pools: To efficiently manage worker threads and avoid the overhead of creating and destroying them for each task, consider using a worker pool library (e.g., workerpool, piscina, poolifier). These libraries manage a reusable pool of workers and assign tasks to available threads.
3. Benchmarking: Always test with different thread counts to find what works best for your specific app and workload.

Libuv Threads vs Node.js Worker Threads :
Both libuv threads and Node.js Worker Threads involve background threads, but they serve different purposes in the Node.js environment.
Worker threads are for CPU-bound tasks that need true parallelism, while the libuv thread pool handles I/O-bound tasks in a non-blocking way. 
In essence, Node.js provides both an automatic, behind-the-scenes thread pool for common asynchronous operations via libuv, and a more explicit, developer-managed mechanism for parallelizing custom CPU-intensive tasks using the worker_threads module. when you need to perform a CPU-intensive task in Node.js, you would use a worker_thread to create a separate thread to handle it, rather than relying on the libuv thread pool. 
worker_threads in Node.js do not directly utilize the libuv thread pool. They are a separate mechanism for achieving true parallelism by creating new operating system threads, each with its own V8 instance and event loop. The libuv thread pool is primarily used for handling I/O-bound operations in the main event loop. 

üîπ Libuv Thread Pool
Purpose:
Used for asynchronous I/O operations that could block the event loop, such as:
1. File system access (fs.readFile)
2. DNS lookups (dns.lookup)
3. Compression (zlib)
4. Some crypto operations (pbkdf2, scrypt)
Mechanism:
When such an operation is triggered, libuv offloads it to its internal thread pool (default size: 4). A worker thread performs the task, and once done, a callback is queued to run on the main thread (event loop).
Scope & Access:
> Internally managed by libuv
> Not accessible or directly controllable via JavaScript
> You can't write or run your own code on these threads

JavaScript Execution (imp):
No ‚Äî JavaScript code always runs on the main thread. These threads only handle native (C++) I/O operations behind the scenes.

üîπ Node.js Worker Threads
Purpose:
Designed for CPU-intensive JavaScript code that would otherwise block the event loop (e.g., image processing, complex calculations).
Mechanism:
Worker threads create independent V8 instances (isolates), each with:
> Its own event loop
> Its own memory space

JavaScript code runs in parallel across these threads. Communication with the main thread happens through message passing (postMessage, MessagePort).

Scope & Access:
> Created and managed explicitly by the developer
> Part of the worker_threads module

JavaScript Execution (imp):
Yes ‚Äî Worker Threads run full JavaScript code independently from the main thread.

+------------------+--------------------------------------+---------------------------------------------+
| Feature          | Libuv Threads                       | Worker Threads                              |
+------------------+-------------------------------------+---------------------------------------------+
| Primary Use Case | Async I/O operations                | CPU-heavy JS computation                    |
| Runs JS Code?    | ‚ùå No (callback only on main thread) | ‚úÖ Yes (full JS execution)                |
| Isolation        | Shared thread pool                  | Separate V8 isolate per worker              |
| Management       | Internal (libuv)                    | Explicit (via worker_threads)               |
| Memory           | Shared with main process            | Own memory space                            |
| Examples         | fs.readFile, dns.lookup             | Custom number crunching, ML inference, etc. |
+------------------+-------------------------------------+---------------------------------------------+

Conclusion:
> Use libuv thread pool (automatically) for I/O-heavy operations ‚Äî no need to manage it.
> Use Worker Threads for CPU-heavy JavaScript ‚Äî when you need true parallelism.

[ctrl+f in Frontend.dox and find "NodeJS is used to code server side. Node JS used V8 engine to run JavaScript code" and paste the below para right after Nodejs para]
V8 Engine :
The V8 engine is an open-source JavaScript and WebAssembly engine developed by Google, primarily written in C++. It is the core component that allows Node.js to execute JavaScript code outside of a web browser environment. 
Key functions and features of V8 within Node.js:
1. JavaScript to Machine Code Compilation:
V8's primary role is to compile JavaScript code directly into native machine code, which the computer's CPU can understand and execute. This process, known as Just-In-Time (JIT) compilation, significantly improves performance compared to interpreting JavaScript line by line.
2. Runtime Environment:
V8 provides the essential runtime environment for JavaScript, managing memory through efficient garbage collection and optimizing code execution with techniques like hidden classes and inline caching.
3. Bridging JavaScript and C++:
Node.js, also written in C++, leverages V8 to provide JavaScript with access to low-level system functionalities that JavaScript alone cannot directly handle. This allows Node.js to interact with the operating system for tasks like file system operations and networking, enabling server-side development.
4. Support for Modern JavaScript Features:
V8 continuously updates to support the latest ECMAScript standards, including features like classes, Promises, and async/await, ensuring Node.js can utilize modern JavaScript syntax and capabilities.
